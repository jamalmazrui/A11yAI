<html>

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:11.55pt;
	margin-left:.5pt;
	text-align:justify;
	text-justify:inter-ideograph;
	text-indent:-.5pt;
	line-height:104%;
	font-size:10.0pt;
	font-family:"Calibri",sans-serif;
	color:#181717;}
h1
	{mso-style-link:"Heading 1 Char";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:.25pt;
	margin-left:.5pt;
	text-indent:-.5pt;
	line-height:103%;
	page-break-after:avoid;
	font-size:10.0pt;
	font-family:"Calibri",sans-serif;
	color:#181717;}
h2
	{mso-style-link:"Heading 2 Char";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:.25pt;
	margin-left:.5pt;
	text-indent:-.5pt;
	line-height:103%;
	page-break-after:avoid;
	font-size:10.0pt;
	font-family:"Calibri",sans-serif;
	color:#181717;}
span.Heading1Char
	{mso-style-name:"Heading 1 Char";
	mso-style-link:"Heading 1";
	font-family:"Calibri",sans-serif;
	color:#181717;
	font-weight:bold;}
span.Heading2Char
	{mso-style-name:"Heading 2 Char";
	mso-style-link:"Heading 2";
	font-family:"Calibri",sans-serif;
	color:#181717;
	font-weight:bold;}
.MsoChpDefault
	{font-size:12.0pt;}
.MsoPapDefault
	{margin-bottom:8.0pt;
	line-height:115%;}
 /* Page Definitions */
 @page WordSection1
	{size:8.5in 11.0in;
	margin:1.25in 148.45pt 39.4pt .75in;}
div.WordSection1
	{page:WordSection1;}
@page WordSection2
	{size:8.5in 11.0in;
	margin:89.25pt 53.95pt 39.4pt .75in;}
div.WordSection2
	{page:WordSection2;}
@page WordSection3
	{size:8.5in 11.0in;
	margin:1.0in .75in 1.0in .75in;}
div.WordSection3
	{page:WordSection3;}
@page WordSection4
	{size:8.5in 11.0in;
	margin:1.25in 53.95pt 39.4pt .75in;}
div.WordSection4
	{page:WordSection4;}
@page WordSection5
	{size:8.5in 11.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection5
	{page:WordSection5;}
 /* List Definitions */
 ol
	{margin-bottom:0in;}
ul
	{margin-bottom:0in;}
-->
</style>

</head>

<body lang=EN-US style='word-wrap:break-word'>

<div class=WordSection1>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:3.95pt;margin-left:0in;text-align:left;text-indent:0in;
line-height:107%'><b><span style='font-size:18.0pt;line-height:107%'>“I WANT”:
AGENCY AND ACCESSIBILITY IN THE AGE OF AI</span></b></p>

<table class=TableGrid border=0 cellspacing=0 cellpadding=0 width=601
 style='width:450.75pt;border-collapse:collapse'>
 <tr style='height:22.2pt'>
  <td width=348 valign=top style='width:261.0pt;padding:0in 0in 0in 0in;
  height:22.2pt'>
  <p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
  margin-bottom:0in;margin-left:.2pt;text-align:left;text-indent:0in;
  line-height:107%'><b>LUIS BORUNDA</b></p>
  <p class=MsoNormal align=left style='margin:0in;text-align:left;text-indent:
  0in;line-height:107%'>Virginia Tech School of Architecture</p>
  </td>
  <td width=253 valign=top style='width:189.75pt;padding:0in 0in 0in 0in;
  height:22.2pt'>
  <p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
  margin-bottom:0in;margin-left:.2pt;text-align:left;text-indent:0in;
  line-height:107%'><b>NA MENG</b></p>
  <p class=MsoNormal style='margin:0in;text-indent:0in;line-height:107%'>Virginia
  Tech Department of Computer Science</p>
  </td>
 </tr>
</table>

<p class=MsoNormal style='margin-top:0in;margin-right:1.5pt;margin-bottom:.25pt;
margin-left:-.05pt;line-height:103%'><b>ANDREW GIPE-LAZAROU</b></p>

<p class=MsoNormal style='margin-top:0in;margin-right:0in;margin-bottom:.35pt;
margin-left:-.25pt'>Virginia Tech School of Architecture </p>

</div>

<span style='font-size:10.0pt;line-height:104%;font-family:"Calibri",sans-serif;
color:#181717'><br clear=all style='page-break-before:auto'>
</span>

<div class=WordSection2>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:12.0pt;margin-left:0in;text-align:left;text-indent:0in;
line-height:100%'><b>Keywords: </b>Human-Centered AI, Disability, Vision
Impairment, Computer Vision, Inclusive Design</p>

<p class=MsoNormal style='margin-top:0in;margin-right:1.5pt;margin-bottom:11.75pt;
margin-left:-.05pt;line-height:103%'><b>“I WANT access to public buildings and
technologies.” “I WANT all stairs to have railings.” “I WANT talking pedestrian
signs.” “I WANT curbs to be more noticeable.” “I WANT technology dedicated to
the blind.” These statements from young, vision-impaired learners participating
in our human-centered research and participatory design initiatives highlight
their desire for more inclusive space-making. This paper investigates the role
of advanced technological interventions and participatory design in shaping the
future of architecture.</b></p>

<p class=MsoNormal style='margin-top:0in;margin-right:1.5pt;margin-bottom:11.75pt;
margin-left:-.05pt;line-height:103%'><b>Navigational autonomy in built
environments is a significant challenge for individuals with vision impairments
and is often exacerbated by traditional design practices that do not adequately
address their specific needs. Through a participatory design approach, this
research integrates direct inputs from vision-impaired participants to guide
the development and refinement of an AI-Accessibility application. We employ
advanced machine learning techniques, including computer vision and natural
language processing, developed and refined through real-world applications at
Virginia Tech’s annual blind design workshop, along with interviews and focus
groups with stakeholders. The feedback gathered from these user experiences was
instrumental in refining the system’s functionality, ensuring its practical
utility and meeting the needs of beneficiaries.</b></p>

<p class=MsoNormal style='margin-top:0in;margin-right:1.5pt;margin-bottom:.25pt;
margin-left:-.05pt;line-height:103%'><img width=48 height=148
src="I%20Want%20Agency%20And%20Accessibility%20In%20The%20Age%20Of%20AI_files/image001.gif"
align=left hspace=12><b>Our research outlines key design principles and
underscores the practical benefits of an emerging disciplinary domain that we
refer to as “AI-Accessibility.” We advocate for a reevaluation of design
education and industry standards, emphasizing a shift toward greater
inclusivity. We propose an equitable approach to technology and design
development, highlighting the essential role of interdisciplinary efforts in
creating accessible public infrastructures with advanced technologies that
enhance the quality of life for all.</b></p>

<h1 style='margin-top:0in;margin-right:.05pt;margin-bottom:.25pt;margin-left:
-.25pt'>1. CONTEMPORARY BARRIERS TO ACCESSIBILITY AND TECHNOLOGICAL GAPS</h1>

<p class=MsoNormal style='margin-left:-.25pt'>In contemporary architectural
practice, integrating emergent technologies, particularly those driven by
artificial intelligence (AI), promises significant enhancements in the
accessibility of built environments. For underserved communities, especially individuals
with vision impairments, navigating daily life poses profound challenges that
are often inadequately addressed by conventional design methodologies. This
paper posits that codesign—engaging community stakeholders in design decisions directly
affecting them, coupled with responsible technological development—can offer
innovative solutions to these challenges. The built environment often fails to
accommodate the diverse needs of vision-impaired individuals, as evidenced by
daily struggles ranging from navigating public spaces to interacting with
inaccessible architectural elements (Figure 1). Despite advancements in
assistive technologies, a critical gap remains in the integration of these
innovations within the broader context of architectural design. This paper
explores how novel AI and codesign methodologies can bridge these deficiencies.</p>

<p class=MsoNormal style='margin-left:-.25pt'>The relevance of this research is
underscored by statistics from the World Health Organization (WHO), indicating
that 2.2 billion people worldwide suffer from vision impairment (Bourne et al.
2017; WHO, 2018). In the U.S., 32.2 million adults reported vision loss, with
184,978 residing in Virginia. The economic burden of vision loss and blindness
in the U.S. resulting from reduced labor force participation totaled $134.2
billion in 2022. These figures highlight the need for innovative solutions to
increase accessibility and independence for vision-impaired individuals.</p>

<p class=MsoNormal style='margin-left:-.25pt'>Case studies of architectural
projects incorporating assistive technologies offer valuable lessons. For
example, The Lighthouse for the Blind and Visually Impaired in San Francisco,
designed by Mark Cavangero Associates and Chris Downey, demonstrates how
thoughtful design can enhance accessibility for vision-impaired individuals.
The Lighthouse incorporates high-contrast chromatic schemes for improved visual
acuity, textured pedestrian pathways with directional ridges for enhanced cane
user stimulation, integrated audio alerts and braille signage, and smart
lighting systems to increase accessibility.</p>

<table class=TableGrid border=0 cellspacing=0 cellpadding=0 align=left
 width=676 style='width:506.8pt;border-collapse:collapse;margin-left:-2.25pt;
 margin-right:-2.25pt'>
 <tr style='height:18.85pt'>
  <td width=607 valign=bottom style='width:455.35pt;padding:0in 2.25pt 0in 0in;
  height:18.85pt'>
  <p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
  margin-bottom:15.95pt;margin-left:0in;text-align:left;text-indent:0in;
  line-height:107%'><span style='font-size:11.0pt;line-height:107%;color:black'><img
  width=673 height=211 id="Group 11286"
  src="I%20Want%20Agency%20And%20Accessibility%20In%20The%20Age%20Of%20AI_files/image002.gif"></span></p>
  <p class=MsoNormal align=left style='margin-top:0in;margin-right:41.6pt;
  margin-bottom:0in;margin-left:.55pt;text-align:left;text-indent:0in;
  line-height:107%'><span style='font-size:8.5pt;line-height:107%'>Figure 1:
  Simulated view of the Guggenheim Museum through the lens of diabetic
  retinopathy. This image illustrates the challenges of low vision and how it
  affects our perception of architecture. Based on Eye Disease Simulation of
  the National Eye Institute.</span></p>
  </td>
 </tr>
</table>

<p class=MsoNormal style='margin-top:0in;margin-right:1.5pt;margin-bottom:.25pt;
margin-left:-.05pt;line-height:103%'><b>1.2. AI INTEGRATION THROUGH
HUMAN-CENTERED DESIGN</b></p>

<p class=MsoNormal style='margin-left:-.25pt'>Audio description demands skills
in observation, editing, language, and vocal presentation to convey visual
content to individuals who are blind or visually impaired, turning describers
into perceptive ‘eyewitnesses’ (Snyder, 2005). Advancements in artificial
intelligence (AI), particularly in natural language processing (NLP), computer
vision (CV), and automatic speech recognition (ASR), open exciting
possibilities for next-generation assistive technologies and inclusive spaces.
According to Budrionis et al. (2020), electronic travel aids (ETAs) can be
classified into sensorbased, camera-based, and hybrid solutions. Zafar et al.
(2022) classified assistive devices into three categories: (1) electronic
orientation aids (EOAs), which are devices that assist users in determining
their location and direction via GPS, digital maps, and spatial information;
(2) electronic mobility aids (EMAs), which are devices that help users detect
and avoid obstacles via sensors such as ultrasonic, infrared, or laser
scanners, providing feedback through auditory or tactile signals; and (3)
position locator devices (PLDs), which are devices that help users pinpoint
their position relative to nearby objects or landmarks and are particularly
useful in unfamiliar or complex environments. Zafar et al. also listed
essential attributes for assistive devices, such as real-time response, large
coverage area, indoor and outdoor functionality, high accuracy, and lightweight
design. Challenges include the need for concise information, reliable
performance, and high accuracy in diverse environments </p>

<p class=MsoNormal style='margin-left:-.25pt'>Among ETAs, specialized mobile
applications for smartphones are increasingly used because of their
portability, cost, easy access to information, and ease of use, providing
greater autonomy for individuals with vision impairments (Griffin-Shirley et
al., 2017). Giudice and Legge (2008) highlight smartphones as crucial hybrid
tools for blind navigation, leveraging built-in sensors and GPS. Ko and Kim
(2023) reviewed advancements in indoor ETAs, emphasizing the potential of
enhanced indoor navigation technologies. Current research has increasingly
applied machine learning tools to complement traditional navigation methods. </p>

<p class=MsoNormal style='margin-left:-.25pt'>Bauer et al. (2020) demonstrated
that deep learning, multisensory feedback, and low-cost wearable sensors can
recreate three-dimensional scenes, significantly enhancing perception for
visually impaired individuals. </p>

<p class=MsoNormal style='margin-left:-.25pt'>While smartphone-hosted computer
vision aids offer costeffective, multifunctional designs with real-time
feedback capabilities, a significant mismatch persists between the practical
needs of users and the focus of academic research on ETAs (Budrionis et al.,
2020). Our contribution to the domain of AI accessibility is grounded in
codesign workshops at Virginia Tech, where vision-impaired participants are
given clear and creative opportunities to articulate their needs, shaping our
research to be user-centric (Figure 2). The primary challenge lies in bridging
the gap between the user-centric functional development of assistive
technologies and the constraints of the physical environment. Ideally, AI
systems should seamlessly interact with and interpret architectural features,
automating the interaction between humans and built spaces. Drawing on insights
from user needs gathered through participatory design workshops, successful
case studies such as the Lighthouse project, and the potential of AI, this
paper advocates for future best practices in inclusive design.</p>

<p class=MsoNormal style='margin-top:0in;margin-right:1.5pt;margin-bottom:.25pt;
margin-left:-.05pt;line-height:103%'><b>2.OBJECTIVES OF THIS STUDY</b></p>

<p class=MsoNormal><span style='line-height:104%'>•<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span>Utilize codesign workshops to understand the needs and
aspirations of vision-impaired individuals, guiding AIAccessibility solutions.</p>

<p class=MsoNormal><span style='line-height:104%'>•<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span>Develop an AI accessibility app Integrating machine learning
algorithms and computer vision techniques to provide real-time environmental
assistance.</p>

<p class=MsoNormal><span style='line-height:104%'>•<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span>Demonstrate and evaluate AI-driven design features within a
collaborative workshop setting.</p>

<p class=MsoNormal><span style='line-height:104%'>•<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span>Discuss the potential impacts and applications of integrating AI
and inclusive codesign into architectural education and practice.</p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:17.6pt;margin-left:-.2pt;text-align:left;text-indent:0in;
line-height:107%'><span style='font-size:11.0pt;line-height:107%;color:black'><img
width=325 height=400 id="Group 10599"
src="I%20Want%20Agency%20And%20Accessibility%20In%20The%20Age%20Of%20AI_files/image003.gif"></span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:18.8pt;margin-left:-.25pt;text-align:left;line-height:103%'><span
style='font-size:8.5pt;line-height:103%'>Figure 2. Participatory design
workshops where vision-impaired learners articulate their needs and desires for
accessible architectural spaces through collaborative discussions and
activities. Images by Andrew Gipe-Lazarou. </span></p>

<h1 style='margin-top:0in;margin-right:.05pt;margin-bottom:.25pt;margin-left:
-.25pt'>3. METHODS</h1>

<p class=MsoNormal style='margin-left:-.25pt'>Generative AI technologies are
poised to restructure humancentered design and accessibility, extending the
capabilities of current techniques by automating complex tasks and generating
high-quality outputs with minimal intervention (McKinsey Global Institute,
2023; Chakraborti, 2020). </p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:12.0pt;margin-left:0in;text-align:left;text-indent:0in;
line-height:100%'>The methodology aims to shape pedagogical strategies,
industry standards, and public policy, promoting a reimagining of inclusive
design education and practice. It involves three key components:</p>

<p class=MsoNormal style='margin-left:-.25pt'>AI Accessibility Integration:
This component focuses on seamlessly integrating AI into the design of
inclusive spaces. Emphasis is placed on user-friendliness, prioritizing the
needs and preferences of vision-impaired individuals. Advanced machine learning
models and computer vision techniques enhance object recognition and
environmental understanding, which are crucial for navigating complex spaces.
Despite advancements in AI systems, concerns about inherent biases persist.
These models, trained on vast corpora of real-world text, often reflect
societal stereotypes and biases (Nadeem, 2020)). Therefore, it is essential to incorporate
codesign strategies to ensure that intelligent process automation effectively
meets user needs.</p>

<p class=MsoNormal style='margin-left:-.25pt'>User-Centered Participatory
Design Methods: Vision-impaired individuals were actively engaged throughout
the design process to gain a deeper understanding of their accessibility
challenges. Codesign workshops and focus groups ensured that the developed solutions
directly addressed the needs and preferences of the target population. The
detailed methodologies are described in Subsection 3.2, Codesign.</p>

<p class=MsoNormal style='margin-left:-.25pt'><img width=48 height=148
src="I%20Want%20Agency%20And%20Accessibility%20In%20The%20Age%20Of%20AI_files/image004.gif"
align=left hspace=12>Iterative development: Regular feedback sessions with
vision-impaired participants were integral to the process. This continuous
feedback loop allowed for ongoing refinement of the system’s usability and
effectiveness. Iterative prototyping and testing phases were conducted
throughout the development process to ensure the practicality and real-world
applicability of the AI solutions. Details on the data analysis are provided in
Subsection 3.3, Data Analysis.</p>

<h2 style='margin-top:0in;margin-right:.05pt;margin-bottom:.25pt;margin-left:
-.25pt'>3.1. COMPUTATIONAL DESIGN</h2>

<p class=MsoNormal style='margin-left:-.25pt'>We iteratively tested systems
that integrate multiple AI technologies to provide comprehensive assistance for
visually impaired users. The system architecture consists of parallel CV
processing layers that capture and analyze different aspects of the environment,
which are subsequently consolidated via LLMs for coherent output.</p>

<p class=MsoNormal style='margin-left:-.25pt'>Machine learning libraries form
the foundation, implementing AI models to interpret and respond to diverse
environments (Figure 3). CV techniques facilitate real-time object recognition,
labeling key elements within the built environment. Simultaneously, depth
estimation algorithms map the spatial layout, whereas pathfinding algorithms
optimize real-time guidance. These parallel processes feed into LLMs, which
consolidate the information to deliver truthful, clear, and dynamic spatial
descriptions. To deliver this integrated, real-time AI-Accessibility
functionality through auditory features with dynamic spatial information, we implemented
the following computational techniques:</p>

<p class=MsoNormal style='margin-left:-.25pt'>1. CV multimodel approach for
comprehensive environmental understanding.</p>

<p class=MsoNormal style='margin-left:-.25pt'>Object Detection and
Segmentation: YOLOv7 and DETR (DEtection TRansformer) were utilized and
fine-tuned on a dataset of 300 images relevant to visually impaired navigation
and scannable 2D Code detection. YOLOv7 provides real-time performance, whereas
DETR offers end-to-end object detection using transformers, which is
particularly effective for complex scene understanding.</p>

<p class=MsoNormal style='margin-left:-.25pt'>Scene Understanding: Implemented
a combination of dense caption, Blip2, and GPT-4 V for detailed scene
description. A dense caption generates multiple captions for various regions
within an image, providing granular information about different elements in the
scene. Blip2, a vision-language pretraining model, enhances our system’s
ability to generate human-like descriptions of visual content. GPT-4 V offered
more contextual and nuanced </p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:17.65pt;margin-left:0in;text-align:left;text-indent:0in;
line-height:107%'><span style='font-size:11.0pt;line-height:107%;color:black'><img
width=324 height=384 id="Group 11285"
src="I%20Want%20Agency%20And%20Accessibility%20In%20The%20Age%20Of%20AI_files/image005.gif"></span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:18.8pt;margin-left:-.25pt;text-align:left;line-height:103%'><span
style='font-size:8.5pt;line-height:103%'>Figure 3. Computer visualization
tools. Image processed using YoloV7 for object recognition (left) and
DepthAnything-V2 for relative depth estimation (right). Image adapted using CV
techniques with permission from Fred Brack, professional audio describer.</span></p>

<p class=MsoNormal style='margin-left:-.25pt'>descriptions of the environment.
This multimodel approach bridges the gap between visual perception and natural
language understanding, providing rich, context-aware descriptions.</p>

<p class=MsoNormal style='margin-left:-.25pt'>Depth Estimation: Employed Depth
Anything V2 and Monodepth2. Depth Anything V2 offers robust depth estimation
across diverse scenes, whereas Monodepth2 provides high-quality depth maps from
single images. We integrated these models to achieve absolute and relative
positioning and spatial mapping.</p>

<p class=MsoNormal><span style='line-height:104%'>2.<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span>Pathfinding: We explored an optimized A* algorithm on a network
graph representation of buildings, aiming to prioritize accessibility
trajectories for efficient route planning. While this approach was intended for
real-time guidance adaptations in complex environments, our current
implementation faced challenges in achieving the desired level of accuracy and
efficiency. This aspect of the system requires further refinement and testing
to meet the needs of users in diverse architectural settings.</p>

<p class=MsoNormal><span style='line-height:104%'>3.<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span>Natural Language Processing (NLP): After capturing contextual
information, we leveraged advanced language models (GPT-3.5, GPT-4, and LLaMA)
enhanced with a) custom prompt engineering based on professional audio
description guidelines; b) retrieval-augmented generation (RAG) using
embeddings from expert audio description examples, including both desired and
undesired cases; and c) insights from focus groups and interview sessions to
refine output quality. This approach applies advanced language models to generate
concise, relevant, professionally styled environmental descriptions and
navigation instructions. By integrating these LLMs with domain-specific
knowledge, the system consolidates and delivers clear, dynamic spatial
information aligned with best practices in audio descriptions for individuals
with vision impairments.</p>

<p class=MsoNormal style='margin-left:-.25pt'>This method enables real-time
interpretation of diverse environments, precise object recognition and
labeling, and provides accurate spatial information. Combined with robust
pathfinding and alerting capabilities, the system offers adaptive guidance for visually
impaired users. The entire system is integrated into an Android application
using Flutter, ensuring a user-friendly interface (Figure 4).</p>

<h2 style='margin-top:0in;margin-right:.05pt;margin-bottom:.25pt;margin-left:
-.25pt'>3.2. CODESIGN</h2>

<p class=MsoNormal style='margin-left:-.25pt'>The integration of AI-driven
navigation systems with user needs and preferences was achieved through
rigorous codesign methodologies, engaging individuals with vision impairment
and key stakeholders in an iterative research and development process. This
process was conducted primarily through annual workshops, customer discovery
interviews, and observational studies.</p>

<p class=MsoNormal style='margin-left:-.25pt'>The participants (n=30)
represented a diverse range of stakeholders, including vision-impaired
individuals (n=15), training center staff, accessibility experts, technology
providers, professional designers, and architecture students. Broad user
priorities were captured through the workshops, whereas specific assistive
technology uses and interactions were examined via focus groups and structured
interviews. The codesign activities included the following:</p>

<p class=MsoNormal><span style='line-height:104%'>1.<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span>The Virginia Tech School of Architecture’s Annual Blind Design
Workshop included analog exercises in drawing and model-making, multisensory
tours of on-campus spaces, demonstrations of 3D printing, and mentorship from
vision-impaired design professionals. These workshops culminated in group
critiques of accessible design proposals.</p>

<p class=MsoNormal><span style='line-height:104%'>2.<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span>Data collection strategy: a) Recorded conversational interviews
(n=30, duration: 30--60 minutes); b) focus group sessions (3--6 participants)
for observational studies</p>

<p class=MsoNormal style='margin-left:-.25pt'>We analyzed the data to identify
and ground recurring themes and user insights. The interview questions probed
navigation strategies, spatial quality preferences, and nonassistive space
perception methods. The interviews ranged from 30 minutes to one hour and
explored questions such as “How do you navigate a public space for the first
time?”, “In a situation where you are navigating to a destination, what spatial
qualities would an audio description ideally focus on?”, and “Please explain
how you perceive or understand space without the use of assistive </p>

</div>

<span style='font-size:10.0pt;line-height:104%;font-family:"Calibri",sans-serif;
color:#181717'><br clear=all style='page-break-before:auto'>
</span>

<div class=WordSection3>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:.4pt;margin-left:-.25pt;text-align:left;line-height:103%'>

<table cellpadding=0 cellspacing=0>
 <tr>
  <td width=72 height=0></td>
 </tr>
 <tr>
  <td></td>
  <td><img width=744 height=844
  src="I%20Want%20Agency%20And%20Accessibility%20In%20The%20Age%20Of%20AI_files/image006.gif"></td>
 </tr>
</table>

<br clear=ALL>
<span style='font-size:8.5pt;line-height:103%'>Figure 4. Components of the
AI-Driven Navigation App displaying three core functionalities: description,
navigation, and Alert to be implemented.</span></p>

</div>

<span style='font-size:10.0pt;line-height:104%;font-family:"Calibri",sans-serif;
color:#181717'><br clear=all style='page-break-before:auto'>
</span>

<div class=WordSection4>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:17.6pt;margin-left:-.3pt;text-align:left;text-indent:0in;
line-height:107%'><span style='font-size:11.0pt;line-height:107%;color:black'><img
width=324 height=256 id="Group 9774"
src="I%20Want%20Agency%20And%20Accessibility%20In%20The%20Age%20Of%20AI_files/image007.gif"></span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:18.8pt;margin-left:-.25pt;text-align:left;line-height:103%'><span
style='font-size:8.5pt;line-height:103%'>Figure 5. Focus Group session, user
feedback and testing workshops with research collaborators Ability2Access,
Virginia Department for the Blind and Visually Impaired and individuals with
vision impairment. Images by Luis Borunda and Andrew Gipe-Lazarou</span></p>

<p class=MsoNormal style='margin-left:-.25pt'>technology”. Observational
studies documented participants’ use of AI tools in a variety of built
environments and during cognitive walkthroughs. These sessions facilitated
iterative refinement of the AI navigation systems on the basis of information
design preferences and identified areas for improvement.</p>

<p class=MsoNormal style='margin-left:-.25pt'>This comprehensive, user-centered
approach ensured that AI solutions addressed existing challenges faced by
vision-impaired individuals, with insights emerging iteratively from the
collected data rather than relying on preconceived assumptions.</p>

<h2 style='margin-top:0in;margin-right:.05pt;margin-bottom:.25pt;margin-left:
-.25pt'>3.3. DATA ANALYSIS</h2>

<p class=MsoNormal style='margin-left:-.25pt'>User interactions with the AI
navigation app were recorded to analyze usage preferences and identify areas
for improvement iteratively (Figure 5). The participants offered qualitative
feedback on prototypes and designs, supplemented by surveys and interviews that
delved deeper into user experiences and suggestions for improvement. Anonymized
demographic information, including age, degree of vision impairment, and prior
experience with assistive technologies, was collected.</p>

<p class=MsoNormal style='margin-left:-.25pt'>We analyzed and identified
participant priorities through structured customer discovery interviews and
focus groups:</p>

<p class=MsoNormal style='margin-left:-.25pt'>The study involved 15 visually
impaired participants, equally divided among those who were blind from birth,
those who were blind later in life, and those with partial vision. All the
participants used white canes, with 60% also relying on sighted guides.
Preferred high-tech aids included Be My Eyes (53%) and Seeing AI (47%). The
majority (93%) used Apple iPhones. For spatial descriptions, 67% preferred
directions to be used alongside the GPS when navigating to a destination,
whereas 40% favored a description that highlights larger landmarks such as
intersections or building elements. Most participants (73%) preferred relative
positioning to absolute positioning. When navigating new spaces, 67% preferred
to ask for assistance, and 60% preferred to use their cane. With respect to
obstacles, two main challenges were identified: sudden drop-offs and overhead hazards,
including low-hanging branches, steps/stairs, and street crossings. For alert
systems, 87% indicated that they would appreciate an automated system, with 53%
preferring audio and spoken-voice alerts.</p>

<p class=MsoNormal style='margin-left:-.25pt'>In the focus groups (Figures 5
and 6), the participants emphasized the need for adaptable, user-friendly
technologies that enhance independence without overwhelming the user; expressed
the need for redundancy in navigation aids; and emphasized the importance of
auditory and tactile cues in understanding spaces. They highlighted the
importance of comprehensive environmental awareness, ease of use, and
information. Concerns about AI included potential accuracy issues and
overreliance on technology.</p>

<h1 style='margin-top:0in;margin-right:.05pt;margin-bottom:.25pt;margin-left:
-.25pt'>4. RESULTS </h1>

<p class=MsoNormal style='margin-left:-.25pt'>The research’s iterative
development resulted in an AI-driven navigation app for Android that provides
real-time environmental descriptions and basic navigation assistance. The
majority of participants reported that the information provided by the app was
valuable and previously unavailable to them. Additionally, the participants
expressed the significance of and interest in the app’s ability to deliver
detailed environmental descriptions. The participants also expressed a
preference for information about the activity of other individuals in a space,
accessible routes, and scenic details (depending on the situation).</p>

<p class=MsoNormal style='margin-left:-.25pt'>The functionality of our project
can be effectively assessed in two categories on the basis of comprehensive
user evaluations:</p>

<p class=MsoNormal style='margin-left:-.25pt'>Accuracy in Environmental
Description: The AI-driven approach was highly accurate in describing
environmental features, showing marked improvement in the rate of correctly
identifying and labeling architectural elements. This capability is critical in
environments where navigational cues heavily depend on precise spatial details.</p>

<p class=MsoNormal style='margin-left:-.25pt'>Reliability of Navigation
Instructions: While the AI-driven navigation instructions performed well in
structured environments, they were found to be less reliable in complex
settings. This underscores the need to simplify computational design and
enhance the predictability of site designs to ensure that AI implementations
are both accurate and practical in realworld applications.</p>

<p class=MsoNormal style='margin-top:0in;margin-right:1.5pt;margin-bottom:.25pt;
margin-left:-.05pt;line-height:103%'><b>4.1. COMPARATIVE ANALYSIS OF AI
ACCESSIBILITY APP DESCRIPTION FEATURES</b></p>

<p class=MsoNormal style='margin-left:-.25pt'>A comparison with
market-available AI-driven tools revealed significant discrepancies in the
reliability of the environmental descriptions. Existing smartphone ETA tools
often provide unreliable descriptions, leading to biased or unsafe user responses.
Our research addressed this issue by enhancing the reliability of the CV tools
used in AI-driven descriptions, ensuring that the environmental information is
accurate, relevant, and safe for users.</p>

<p class=MsoNormal style='margin-left:-.25pt'>Example 1: A state-of-the-art app
that is asked to describe Figure 5 (bottom right) yields “25-year-old woman
with dark hair and neutral expression” — an example of unreliable and
irrelevant information that does not assist in navigation or spatial awareness.</p>

<p class=MsoNormal style='margin-left:-.25pt'>Example 2: Another app described
a “Door with a blue curtain in front” — offering limited information for
navigating the layout of an unfamiliar setting (describing Figure 6 scene).</p>

<p class=MsoNormal style='margin-left:-.25pt'>Example 3: Our SmartGuide App was
evaluated across various Android OS system architectures via a multistep
approach, leading to the identification of key enhancements. Our testing
suggests that incorporating LLM and CV without valid scene understanding can
lead to highly inaccurate descriptions, as seen in the following examples:</p>

<p class=MsoNormal style='margin-left:-.25pt'>“In the bathroom, a blue curtain
adorns the window, infusing the space with tranquil hues. A toilet rests on the
wooden floor, a white and chrome chair offers a spot for relaxation. Nearby, a
black trash can with a white liner discreetly manages waste. Beyond, a hallway
connects to the rest of the building, leading to various destinations.” (App
Version 1: Unreliable LLM integration, while partially descriptive, lacks
crucial details and provides false information).</p>

<p class=MsoNormal style='margin-left:-.25pt'>“The image shows an indoor room
corner with a door at the center, fitted with a window that has a
blue&#8210;gray curtain tied in the middle. To the left, there is a small white
table with various papers and a sanitizer bottle on top, above which is a suspended
ceiling with a speaker. A step ladder, a trash can, and some informational
pamphlets on the wall are also visible. To the right, there is a wall-mounted
brochure holder with leaflets. The floor is a dark wood laminate. The setting
appears to be a professional office or clinic waiting area” (App Version 2).</p>

<p class=MsoNormal style='margin-left:-.25pt'><img width=396 height=257
src="I%20Want%20Agency%20And%20Accessibility%20In%20The%20Age%20Of%20AI_files/image008.gif"
align=left hspace=12>Enhanced descriptive accuracy with LLMs: Our studies show
that incorporating LLMs allows us to leverage prompt engineering and RAG
techniques to craft prompts that result in engaging and informative
descriptions. The incorporation of GPT-4 and LLaMA LLMs improved the
reliability and richness of descriptions, and the addition of multistep object
detection and scene understanding tools allowed us to better align description
outputs with priorities and needs specific to individuals with visual
impairment. Through codesign workshops, the prompts and therefore the output
descriptions were tailored to be more engaging, useful, and appealing to
visually impaired users, ensuring the provision of valuable spatial information
shaped by individual preferences.</p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:18.8pt;margin-left:-.25pt;text-align:left;line-height:103%'><span
style='font-size:8.5pt;line-height:103%'>Figure 6. Testing of AI navigation
systems in codesign environments with research collaborators Ability2Access,
Virginia Department for the Blind and Visually Impaired and individuals with
vision impairment. Images by Andrew Gipe-Lazarou</span></p>

<h1 style='margin-top:0in;margin-right:.05pt;margin-bottom:.25pt;margin-left:
-.25pt'>5. CONTRIBUTIONS AND EMPIRICAL FINDINGS</h1>

<p class=MsoNormal style='margin-left:-.25pt'>This research substantiates the
transformative integration of AI-driven navigation systems into accessible
environments, anticipating significant shifts in architectural practice, policy
formulation, and design pedagogy. Using AI-powered accessibility tools
alongside participatory design methodologies, this study augmented the
navigational capabilities and spatial autonomy of vision-impaired individuals;
it deployed adaptive navigation systems that were refined iteratively by
extensive user feedback and codesign principles, demonstrating marked
enhancements in the users’ capacity to comprehend their surroundings, navigate
effectively, and interact with their environment.</p>

<p class=MsoNormal style='margin-left:-.25pt'>We identify two broader impacts:</p>

<p class=MsoNormal style='margin-left:-.25pt'>Impact on Design Practice and
Education: The methodological framework of codesign workshops has deepened our
understanding of the accessibility challenges inherent in current architectural
pedagogy and practice. Direct interactions with vision-impaired participants
have provided critical insights into the deficiencies of existing designs and
identified innovative opportunities for improvement. This process has extended
the educational scope of our work beyond this project, enriching the design
community’s comprehension of inclusive principles and advocating their
integration into professional curricula and practices.</p>

<p class=MsoNormal style='margin-left:-.25pt'>Design for AI Accessibility: Our
research employed advanced machine learning models and computer vision
techniques to significantly improve object recognition and environmental
understanding. These advancements improve the accessibility of built environments,
particularly for visually impaired individuals. We optimized the technologies
through iterative testing and multimodel integration and provided real-time
feedback through auditory and haptic outputs, significantly augmenting spatial
awareness for vision-impaired users. Despite these advancements, the full
integration of these AI systems into architectural designs remains a complex
challenge, requiring further adaptation to ensure their seamless functionality
and inclusivity.</p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:.05pt;
margin-bottom:.25pt;margin-left:-.25pt;text-align:left;line-height:103%'><b>5.1.
FUTURE DIRECTIONS AND CONTINUED RESEARCH:</b></p>

<p class=MsoNormal style='margin-left:-.25pt'>The ongoing refinement of the
AI-driven navigation app through participatory design workshops will increase
our comprehension of inclusive architectural strategies, fostering more robust
engagement between stakeholders and design professionals. The research aims to
broaden the scope of these technologies to encompass a wider range of
disabilities, enhancing their utility and inclusiveness. Navigation in complex
environments requires robust, expensive AI systems.</p>

<p class=MsoNormal style='margin-left:-.25pt'>These systems must process large
volumes of data to manage real-time changes and obstacles. A significant
challenge is the seamless integration of AI technologies into diverse
architectural environments via cost-efficient, generalizable techniques. Ensuring
robustness and adaptability across various settings necessitates ongoing
research and development. Despite advancements in generative AI, inherent
biases remain a concern, as these models often reflect societal stereotypes
from the vast corpora of real-world text on which they are trained. Their
integration into intelligent process automation can benefit greatly from
participatory design strategies. Additionally, expanding the scope of these
systems to accommodate a broader range of disabilities will enhance their
inclusivity and utility. The AI accessibility systems developed in this study
significantly improve the accessibility of built environments for
vision-impaired individuals. The high accuracy and reliability of contextual
descriptions and simple navigation instructions, as indicated by user feedback,
underscore their potential to transform how vision-impaired individuals
interact with and experience their surroundings. Future efforts will focus on
simplifying the integration of AI technologies into both new and existing
architectural frameworks, enhancing cost-effectiveness and operational
efficiency.</p>

<h1 style='margin-top:0in;margin-right:.05pt;margin-bottom:.25pt;margin-left:
-.25pt'>6. CONCLUSION</h1>

<p class=MsoNormal style='margin-left:-.25pt'>This study advances the design
and integration of artificial intelligence (AI) systems into architectural
practice to improve the accessibility of the built environment for individuals
with vision impairment. Our collaborative efforts among architects, technologists,
students, and community stakeholders have resulted in foundational principles
for the codesign of AI-enabled systems that enhance the functional independence
of users with disabilities.</p>

<p class=MsoNormal style='margin-left:-.25pt'>This research has demonstrated
emerging methods for accessing previously inaccessible spaces that are scalable
and cost-effective. The results of our work encourage ongoing partnerships
between the disability community and professionals in architecture and
technology. Such collaboration is essential to ensure that AI technologies meet
both the technical standards and practical needs of people with disabilities.
Our findings highlight the importance of multidisciplinary collaboration in
creating solutions that are innovative and inclusive. Looking ahead, it is
crucial that these collaborations continue to grow, ensuring that accessibility
is prioritized from a project’s conception to its completion.</p>

<h1 style='margin-top:0in;margin-right:.05pt;margin-bottom:11.3pt;margin-left:
-.25pt'><span style='font-weight:normal'> </span>7. ACKNOWLEDGEMENTS</h1>

<p class=MsoNormal style='margin-top:0in;margin-right:0in;margin-bottom:7.4pt;
margin-left:-.25pt'>Our research team would like to extend its gratitude to the
Virginia Department for the Blind and Vision Impaired (DBVI), the Institute for
Creativity, Arts, and Technology (ICAT) at Virginia Tech, the New River Valley
Disability Resource Center (NRVDRC), and Ability2Access for their invaluable
contributions to this research. This work was supported by the Virginia Tech
Center for New Ventures and the Commonwealth Cyber Initiative (CCI Northern
Virginia Node). Relevant data, models, or codes that support the findings of
this study are available from the corresponding author upon reasonable request.
The authors contributed equally to this study. </p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:7.65pt;margin-left:0in;text-align:left;text-indent:0in;
line-height:107%'><span style='font-size:11.0pt;line-height:107%;color:black'><img
width=325 height=1 id="Group 10118"
src="I%20Want%20Agency%20And%20Accessibility%20In%20The%20Age%20Of%20AI_files/image009.gif"></span></p>

<h2 style='margin-top:0in;margin-right:0in;margin-bottom:1.4pt;margin-left:
.15pt;text-indent:0in;line-height:107%'><span style='font-size:8.0pt;
line-height:107%'>ENDNOTES</span></h2>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:4.25pt;margin-left:17.0pt;text-align:left;text-indent:-17.0pt;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>1.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><span style='font-size:7.0pt;line-height:90%'>Bauer, Zuria, et
al. “Enhancing Perception for the Visually Impaired with Deep Learning
Techniques and Low-Cost Wearable Sensors.” Pattern Recognition Letters 137, no.
3 (September 2019): 27-36. DOI:10.1016/j.patrec.2019.03.008.</span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:4.25pt;margin-left:17.0pt;text-align:left;text-indent:-17.0pt;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>2.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><span style='font-size:7.0pt;line-height:90%'>Bourne, Rupert
R.A., et al. “Magnitude, Temporal Trends, and Projections of the Global
Prevalence of Blindness and Distance and Near Vision Impairment: A Systematic
Review and Meta-Analysis.” The Lancet Global Health 5, no. 9 (September 2017):
e888-e897. DOI:10.1016/S2214-109X(17)30293-0.</span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:4.25pt;margin-left:17.0pt;text-align:left;text-indent:-17.0pt;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>3.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><span style='font-size:7.0pt;line-height:90%'>Budrionis, Andrius,
et al. “Smartphone-Based Computer Vision Travelling Aids for Blind and Visually
Impaired Individuals: A Systematic Review.” Assistive Technology 34, no. 2
(2020): 178-194. DOI:10.1080/10400435.2020.1743381.</span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:1.25pt;margin-left:17.0pt;text-align:left;text-indent:-17.0pt;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>4.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><span style='font-size:7.0pt;line-height:90%'>Chakraborti,
Tathagata, et al. “From Robotic Process Automation to Intelligent </span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:4.25pt;margin-left:17.0pt;text-align:left;text-indent:0in;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>Process
Automation,” in Business Process Management: Blockchain and Robotic Process
Automation Forum, ed. Aleksandre Asatiani, Josep Carmona, and Remco Dijkman
(Cham: Springer, 2020), 107-121.</span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:4.25pt;margin-left:17.0pt;text-align:left;text-indent:-17.0pt;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>5.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><span style='font-size:7.0pt;line-height:90%'>Chui, Michael, et
al. The Economic Potential of Generative AI: The Next Productivity Frontier
(New York: McKinsey Global Institute, 2023).</span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:4.25pt;margin-left:17.0pt;text-align:left;text-indent:-17.0pt;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>6.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><span style='font-size:7.0pt;line-height:90%'>Giudice, Nicholas
A., and Gordon E. Legge, “Blind Navigation and the Role of Technology,” in
Engineering Handbook of Smart Technology for Aging, Disability, and
Independence, ed. Abdelsalam Helal, Mounir Mokhtari, and Bessam Abdulrazak
(Hoboken: John Wiley &amp; Sons, 2008), 479-500.</span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:4.25pt;margin-left:17.0pt;text-align:left;text-indent:-17.0pt;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>7.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><span style='font-size:7.0pt;line-height:90%'>Griffin-Shirley,
Nora, et al. “A Survey on the Use of Mobile Applications for People Who are
Visually Impaired.” Journal of Visual Impairment &amp; Blindness 114, no. 4
(July 2017): 307-323.</span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:4.25pt;margin-left:17.0pt;text-align:left;text-indent:-17.0pt;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>8.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><span style='font-size:7.0pt;line-height:90%'>Kim, In-Ju. “Recent
Advancements in Indoor Electronic Travel Aids for the Blind or Visually
Impaired: A Comprehensive Review of Technologies and Implementations.”
Universal Access in the Information Society (2023).
DOI:10.1007/s10209-023-01086-8.</span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:4.25pt;margin-left:17.0pt;text-align:left;text-indent:-17.0pt;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>9.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><span style='font-size:7.0pt;line-height:90%'>Ko, Eunjeong, and
Eun Yi Kim. “A Vision-Based Wayfinding System for Visually Impaired People
Using Situation Awareness and Activity-Based Instructions.” Sensors 17, no. 8
(August 2017). DOI:10.3390/s17081882.</span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:1.25pt;margin-left:17.0pt;text-align:left;text-indent:-17.0pt;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>10.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><span
style='font-size:7.0pt;line-height:90%'>Nadeem, Moin, Anna Bethke, and Siva
Reddy. “StereoSet: Measuring </span></p>

<p class=MsoNormal align=center style='margin-top:0in;margin-right:3.5pt;
margin-bottom:0in;margin-left:0in;text-align:center;text-indent:0in;line-height:
107%'><span style='font-size:7.0pt;line-height:107%'>Stereotypical Bias in
Pretrained Language Models,” in Proceedings of the </span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:1.25pt;margin-left:17.0pt;text-align:left;text-indent:0in;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>59th Annual
Meeting of the Association for Computational Linguistics and the </span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:4.25pt;margin-left:17.0pt;text-align:left;text-indent:0in;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>11th
International Joint Conference on Natural Language Processing (Online:
Association for Computational Linguistics, 2021), 5356-5371.</span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:4.25pt;margin-left:17.0pt;text-align:left;text-indent:-17.0pt;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>11.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><span
style='font-size:7.0pt;line-height:90%'>Snyder, Joel. “Audio Description:
Seeing with the Mind’s Eye: A Comprehensive Training Manual and Guide to the
History and Applications of Audio Description.” PhD diss., Universitat Autònoma
de Barcelona, 2014.</span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:4.25pt;margin-left:17.0pt;text-align:left;text-indent:-17.0pt;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>12.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><span
style='font-size:7.0pt;line-height:90%'>World Health Organization. World Report
on Vision (Geneva: World Health Organization, 2019).
https://www.who.int/publications/i/item/9789241516570.</span></p>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:0in;
margin-bottom:4.25pt;margin-left:17.0pt;text-align:left;text-indent:-17.0pt;
line-height:90%'><span style='font-size:7.0pt;line-height:90%'>13.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><span
style='font-size:7.0pt;line-height:90%'>Zafar, Sadia, et al. “Assistive Devices
Analysis for Visually Impaired Persons: A Review on Taxonomy.” IEEE Access 10
(2022): 13354-13366. DOI:10.1109/ access.2022.3146728.</span></p>

</div>

<span style='font-size:10.0pt;line-height:104%;font-family:"Calibri",sans-serif;
color:#181717'><br clear=all style='page-break-before:auto'>
</span>

<div class=WordSection5>

<p class=MsoNormal align=left style='margin-top:0in;margin-right:7.5in;
margin-bottom:0in;margin-left:-1.0in;text-align:left;text-indent:0in;
line-height:107%'>

<table cellpadding=0 cellspacing=0 align=left>
 <tr>
  <td width=768 height=0></td>
 </tr>
 <tr>
  <td></td>
  <td><img width=48 height=148
  src="I%20Want%20Agency%20And%20Accessibility%20In%20The%20Age%20Of%20AI_files/image010.gif"></td>
 </tr>
</table>

<br clear=ALL>
</p>

</div>

</body>

</html>
