"I WANT": AGENCY AND ACCESSIBILITY IN THE AGE OF AI
LUIS BORUNDA
Virginia Tech School of ArchitectureNA MENG
Virginia Tech Department of Computer ScienceANDREW GIPE-LAZAROU
Virginia Tech School of Architecture 

Keywords: Human-Centered AI, Disability, Vision Impairment, Computer Vision, Inclusive Design
"I WANT access to public buildings and technologies." "I WANT all stairs to have railings." "I WANT talking pedestrian signs." "I WANT curbs to be more noticeable." "I WANT technology dedicated to the blind." These statements from young, vision-impaired learners participating in our human-centered research and participatory design initiatives highlight their desire for more inclusive space-making. This paper investigates the role of advanced technological interventions and participatory design in shaping the future of architecture.
Navigational autonomy in built environments is a significant challenge for individuals with vision impairments and is often exacerbated by traditional design practices that do not adequately address their specific needs. Through a participatory design approach, this research integrates direct inputs from vision-impaired participants to guide the development and refinement of an AI-Accessibility application. We employ advanced machine learning techniques, including computer vision and natural language processing, developed and refined through real-world applications at Virginia Tech's annual blind design workshop, along with interviews and focus groups with stakeholders. The feedback gathered from these user experiences was instrumental in refining the system's functionality, ensuring its practical utility and meeting the needs of beneficiaries.
Our research outlines key design principles and underscores the practical benefits of an emerging disciplinary domain that we refer to as "AI-Accessibility." We advocate for a reevaluation of design education and industry standards, emphasizing a shift toward greater inclusivity. We propose an equitable approach to technology and design development, highlighting the essential role of interdisciplinary efforts in creating accessible public infrastructures with advanced technologies that enhance the quality of life for all.
1. CONTEMPORARY BARRIERS TO ACCESSIBILITY AND TECHNOLOGICAL GAPS
In contemporary architectural practice, integrating emergent technologies, particularly those driven by artificial intelligence (AI), promises significant enhancements in the accessibility of built environments. For underserved communities, especially individuals with vision impairments, navigating daily life poses profound challenges that are often inadequately addressed by conventional design methodologies. This paper posits that codesign-engaging community stakeholders in design decisions directly affecting them, coupled with responsible technological development-can offer innovative solutions to these challenges. The built environment often fails to accommodate the diverse needs of vision-impaired individuals, as evidenced by daily struggles ranging from navigating public spaces to interacting with inaccessible architectural elements (Figure 1). Despite advancements in assistive technologies, a critical gap remains in the integration of these innovations within the broader context of architectural design. This paper explores how novel AI and codesign methodologies can bridge these deficiencies.
The relevance of this research is underscored by statistics from the World Health Organization (WHO), indicating that 2.2 billion people worldwide suffer from vision impairment (Bourne et al. 2017; WHO, 2018). In the U.S., 32.2 million adults reported vision loss, with 184,978 residing in Virginia. The economic burden of vision loss and blindness in the U.S. resulting from reduced labor force participation totaled $134.2 billion in 2022. These figures highlight the need for innovative solutions to increase accessibility and independence for vision-impaired individuals.
Case studies of architectural projects incorporating assistive technologies offer valuable lessons. For example, The Lighthouse for the Blind and Visually Impaired in San Francisco, designed by Mark Cavangero Associates and Chris Downey, demonstrates how thoughtful design can enhance accessibility for vision-impaired individuals. The Lighthouse incorporates high-contrast chromatic schemes for improved visual acuity, textured pedestrian pathways with directional ridges for enhanced cane user stimulation, integrated audio alerts and braille signage, and smart lighting systems to increase accessibility.

Figure 1: Simulated view of the Guggenheim Museum through the lens of diabetic retinopathy. This image illustrates the challenges of low vision and how it affects our perception of architecture. Based on Eye Disease Simulation of the National Eye Institute.1.2. AI INTEGRATION THROUGH HUMAN-CENTERED DESIGN
Audio description demands skills in observation, editing, language, and vocal presentation to convey visual content to individuals who are blind or visually impaired, turning describers into perceptive 'eyewitnesses' (Snyder, 2005). Advancements in artificial intelligence (AI), particularly in natural language processing (NLP), computer vision (CV), and automatic speech recognition (ASR), open exciting possibilities for next-generation assistive technologies and inclusive spaces. According to Budrionis et al. (2020), electronic travel aids (ETAs) can be classified into sensorbased, camera-based, and hybrid solutions. Zafar et al. (2022) classified assistive devices into three categories: (1) electronic orientation aids (EOAs), which are devices that assist users in determining their location and direction via GPS, digital maps, and spatial information; (2) electronic mobility aids (EMAs), which are devices that help users detect and avoid obstacles via sensors such as ultrasonic, infrared, or laser scanners, providing feedback through auditory or tactile signals; and (3) position locator devices (PLDs), which are devices that help users pinpoint their position relative to nearby objects or landmarks and are particularly useful in unfamiliar or complex environments. Zafar et al. also listed essential attributes for assistive devices, such as real-time response, large coverage area, indoor and outdoor functionality, high accuracy, and lightweight design. Challenges include the need for concise information, reliable performance, and high accuracy in diverse environments 
Among ETAs, specialized mobile applications for smartphones are increasingly used because of their portability, cost, easy access to information, and ease of use, providing greater autonomy for individuals with vision impairments (Griffin-Shirley et al., 2017). Giudice and Legge (2008) highlight smartphones as crucial hybrid tools for blind navigation, leveraging built-in sensors and GPS. Ko and Kim (2023) reviewed advancements in indoor ETAs, emphasizing the potential of enhanced indoor navigation technologies. Current research has increasingly applied machine learning tools to complement traditional navigation methods. 
Bauer et al. (2020) demonstrated that deep learning, multisensory feedback, and low-cost wearable sensors can recreate three-dimensional scenes, significantly enhancing perception for visually impaired individuals. 
While smartphone-hosted computer vision aids offer costeffective, multifunctional designs with real-time feedback capabilities, a significant mismatch persists between the practical needs of users and the focus of academic research on ETAs (Budrionis et al., 2020). Our contribution to the domain of AI accessibility is grounded in codesign workshops at Virginia Tech, where vision-impaired participants are given clear and creative opportunities to articulate their needs, shaping our research to be user-centric (Figure 2). The primary challenge lies in bridging the gap between the user-centric functional development of assistive technologies and the constraints of the physical environment. Ideally, AI systems should seamlessly interact with and interpret architectural features, automating the interaction between humans and built spaces. Drawing on insights from user needs gathered through participatory design workshops, successful case studies such as the Lighthouse project, and the potential of AI, this paper advocates for future best practices in inclusive design.
2.OBJECTIVES OF THIS STUDY
• Utilize codesign workshops to understand the needs and aspirations of vision-impaired individuals, guiding AIAccessibility solutions.
• Develop an AI accessibility app Integrating machine learning algorithms and computer vision techniques to provide real-time environmental assistance.
• Demonstrate and evaluate AI-driven design features within a collaborative workshop setting.
• Discuss the potential impacts and applications of integrating AI and inclusive codesign into architectural education and practice.

Figure 2. Participatory design workshops where vision-impaired learners articulate their needs and desires for accessible architectural spaces through collaborative discussions and activities. Images by Andrew Gipe-Lazarou. 
3. METHODS
Generative AI technologies are poised to restructure humancentered design and accessibility, extending the capabilities of current techniques by automating complex tasks and generating high-quality outputs with minimal intervention (McKinsey Global Institute, 2023; Chakraborti, 2020). 
The methodology aims to shape pedagogical strategies, industry standards, and public policy, promoting a reimagining of inclusive design education and practice. It involves three key components:
AI Accessibility Integration: This component focuses on seamlessly integrating AI into the design of inclusive spaces. Emphasis is placed on user-friendliness, prioritizing the needs and preferences of vision-impaired individuals. Advanced machine learning models and computer vision techniques enhance object recognition and environmental understanding, which are crucial for navigating complex spaces. Despite advancements in AI systems, concerns about inherent biases persist. These models, trained on vast corpora of real-world text, often reflect societal stereotypes and biases (Nadeem, 2020)). Therefore, it is essential to incorporate codesign strategies to ensure that intelligent process automation effectively meets user needs.
User-Centered Participatory Design Methods: Vision-impaired individuals were actively engaged throughout the design process to gain a deeper understanding of their accessibility challenges. Codesign workshops and focus groups ensured that the developed solutions directly addressed the needs and preferences of the target population. The detailed methodologies are described in Subsection 3.2, Codesign.
Iterative development: Regular feedback sessions with vision-impaired participants were integral to the process. This continuous feedback loop allowed for ongoing refinement of the system's usability and effectiveness. Iterative prototyping and testing phases were conducted throughout the development process to ensure the practicality and real-world applicability of the AI solutions. Details on the data analysis are provided in Subsection 3.3, Data Analysis.
3.1. COMPUTATIONAL DESIGN
We iteratively tested systems that integrate multiple AI technologies to provide comprehensive assistance for visually impaired users. The system architecture consists of parallel CV processing layers that capture and analyze different aspects of the environment, which are subsequently consolidated via LLMs for coherent output.
Machine learning libraries form the foundation, implementing AI models to interpret and respond to diverse environments (Figure 3). CV techniques facilitate real-time object recognition, labeling key elements within the built environment. Simultaneously, depth estimation algorithms map the spatial layout, whereas pathfinding algorithms optimize real-time guidance. These parallel processes feed into LLMs, which consolidate the information to deliver truthful, clear, and dynamic spatial descriptions. To deliver this integrated, real-time AI-Accessibility functionality through auditory features with dynamic spatial information, we implemented the following computational techniques:
1. CV multimodel approach for comprehensive environmental understanding.
Object Detection and Segmentation: YOLOv7 and DETR (DEtection TRansformer) were utilized and fine-tuned on a dataset of 300 images relevant to visually impaired navigation and scannable 2D Code detection. YOLOv7 provides real-time performance, whereas DETR offers end-to-end object detection using transformers, which is particularly effective for complex scene understanding.
Scene Understanding: Implemented a combination of dense caption, Blip2, and GPT-4 V for detailed scene description. A dense caption generates multiple captions for various regions within an image, providing granular information about different elements in the scene. Blip2, a vision-language pretraining model, enhances our system's ability to generate human-like descriptions of visual content. GPT-4 V offered more contextual and nuanced 

Figure 3. Computer visualization tools. Image processed using YoloV7 for object recognition (left) and DepthAnything-V2 for relative depth estimation (right). Image adapted using CV techniques with permission from Fred Brack, professional audio describer.
descriptions of the environment. This multimodel approach bridges the gap between visual perception and natural language understanding, providing rich, context-aware descriptions.
Depth Estimation: Employed Depth Anything V2 and Monodepth2. Depth Anything V2 offers robust depth estimation across diverse scenes, whereas Monodepth2 provides high-quality depth maps from single images. We integrated these models to achieve absolute and relative positioning and spatial mapping.
2. Pathfinding: We explored an optimized A* algorithm on a network graph representation of buildings, aiming to prioritize accessibility trajectories for efficient route planning. While this approach was intended for real-time guidance adaptations in complex environments, our current implementation faced challenges in achieving the desired level of accuracy and efficiency. This aspect of the system requires further refinement and testing to meet the needs of users in diverse architectural settings.
3. Natural Language Processing (NLP): After capturing contextual information, we leveraged advanced language models (GPT-3.5, GPT-4, and LLaMA) enhanced with a) custom prompt engineering based on professional audio description guidelines; b) retrieval-augmented generation (RAG) using embeddings from expert audio description examples, including both desired and undesired cases; and c) insights from focus groups and interview sessions to refine output quality. This approach applies advanced language models to generate concise, relevant, professionally styled environmental descriptions and navigation instructions. By integrating these LLMs with domain-specific knowledge, the system consolidates and delivers clear, dynamic spatial information aligned with best practices in audio descriptions for individuals with vision impairments.
This method enables real-time interpretation of diverse environments, precise object recognition and labeling, and provides accurate spatial information. Combined with robust pathfinding and alerting capabilities, the system offers adaptive guidance for visually impaired users. The entire system is integrated into an Android application using Flutter, ensuring a user-friendly interface (Figure 4).
3.2. CODESIGN
The integration of AI-driven navigation systems with user needs and preferences was achieved through rigorous codesign methodologies, engaging individuals with vision impairment and key stakeholders in an iterative research and development process. This process was conducted primarily through annual workshops, customer discovery interviews, and observational studies.
The participants (n=30) represented a diverse range of stakeholders, including vision-impaired individuals (n=15), training center staff, accessibility experts, technology providers, professional designers, and architecture students. Broad user priorities were captured through the workshops, whereas specific assistive technology uses and interactions were examined via focus groups and structured interviews. The codesign activities included the following:
1. The Virginia Tech School of Architecture's Annual Blind Design Workshop included analog exercises in drawing and model-making, multisensory tours of on-campus spaces, demonstrations of 3D printing, and mentorship from vision-impaired design professionals. These workshops culminated in group critiques of accessible design proposals.
2. Data collection strategy: a) Recorded conversational interviews (n=30, duration: 30--60 minutes); b) focus group sessions (3--6 participants) for observational studies
We analyzed the data to identify and ground recurring themes and user insights. The interview questions probed navigation strategies, spatial quality preferences, and nonassistive space perception methods. The interviews ranged from 30 minutes to one hour and explored questions such as "How do you navigate a public space for the first time?", "In a situation where you are navigating to a destination, what spatial qualities would an audio description ideally focus on?", and "Please explain how you perceive or understand space without the use of assistive 

Figure 4. Components of the AI-Driven Navigation App displaying three core functionalities: description, navigation, and Alert to be implemented.


Figure 5. Focus Group session, user feedback and testing workshops with research collaborators Ability2Access, Virginia Department for the Blind and Visually Impaired and individuals with vision impairment. Images by Luis Borunda and Andrew Gipe-Lazarou
technology". Observational studies documented participants' use of AI tools in a variety of built environments and during cognitive walkthroughs. These sessions facilitated iterative refinement of the AI navigation systems on the basis of information design preferences and identified areas for improvement.
This comprehensive, user-centered approach ensured that AI solutions addressed existing challenges faced by vision-impaired individuals, with insights emerging iteratively from the collected data rather than relying on preconceived assumptions.
3.3. DATA ANALYSIS
User interactions with the AI navigation app were recorded to analyze usage preferences and identify areas for improvement iteratively (Figure 5). The participants offered qualitative feedback on prototypes and designs, supplemented by surveys and interviews that delved deeper into user experiences and suggestions for improvement. Anonymized demographic information, including age, degree of vision impairment, and prior experience with assistive technologies, was collected.
We analyzed and identified participant priorities through structured customer discovery interviews and focus groups:
The study involved 15 visually impaired participants, equally divided among those who were blind from birth, those who were blind later in life, and those with partial vision. All the participants used white canes, with 60% also relying on sighted guides. Preferred high-tech aids included Be My Eyes (53%) and Seeing AI (47%). The majority (93%) used Apple iPhones. For spatial descriptions, 67% preferred directions to be used alongside the GPS when navigating to a destination, whereas 40% favored a description that highlights larger landmarks such as intersections or building elements. Most participants (73%) preferred relative positioning to absolute positioning. When navigating new spaces, 67% preferred to ask for assistance, and 60% preferred to use their cane. With respect to obstacles, two main challenges were identified: sudden drop-offs and overhead hazards, including low-hanging branches, steps/stairs, and street crossings. For alert systems, 87% indicated that they would appreciate an automated system, with 53% preferring audio and spoken-voice alerts.
In the focus groups (Figures 5 and 6), the participants emphasized the need for adaptable, user-friendly technologies that enhance independence without overwhelming the user; expressed the need for redundancy in navigation aids; and emphasized the importance of auditory and tactile cues in understanding spaces. They highlighted the importance of comprehensive environmental awareness, ease of use, and information. Concerns about AI included potential accuracy issues and overreliance on technology.
4. RESULTS 
The research's iterative development resulted in an AI-driven navigation app for Android that provides real-time environmental descriptions and basic navigation assistance. The majority of participants reported that the information provided by the app was valuable and previously unavailable to them. Additionally, the participants expressed the significance of and interest in the app's ability to deliver detailed environmental descriptions. The participants also expressed a preference for information about the activity of other individuals in a space, accessible routes, and scenic details (depending on the situation).
The functionality of our project can be effectively assessed in two categories on the basis of comprehensive user evaluations:
Accuracy in Environmental Description: The AI-driven approach was highly accurate in describing environmental features, showing marked improvement in the rate of correctly identifying and labeling architectural elements. This capability is critical in environments where navigational cues heavily depend on precise spatial details.
Reliability of Navigation Instructions: While the AI-driven navigation instructions performed well in structured environments, they were found to be less reliable in complex settings. This underscores the need to simplify computational design and enhance the predictability of site designs to ensure that AI implementations are both accurate and practical in realworld applications.
4.1. COMPARATIVE ANALYSIS OF AI ACCESSIBILITY APP DESCRIPTION FEATURES
A comparison with market-available AI-driven tools revealed significant discrepancies in the reliability of the environmental descriptions. Existing smartphone ETA tools often provide unreliable descriptions, leading to biased or unsafe user responses. Our research addressed this issue by enhancing the reliability of the CV tools used in AI-driven descriptions, ensuring that the environmental information is accurate, relevant, and safe for users.
Example 1: A state-of-the-art app that is asked to describe Figure 5 (bottom right) yields "25-year-old woman with dark hair and neutral expression" - an example of unreliable and irrelevant information that does not assist in navigation or spatial awareness.
Example 2: Another app described a "Door with a blue curtain in front" - offering limited information for navigating the layout of an unfamiliar setting (describing Figure 6 scene).
Example 3: Our SmartGuide App was evaluated across various Android OS system architectures via a multistep approach, leading to the identification of key enhancements. Our testing suggests that incorporating LLM and CV without valid scene understanding can lead to highly inaccurate descriptions, as seen in the following examples:
"In the bathroom, a blue curtain adorns the window, infusing the space with tranquil hues. A toilet rests on the wooden floor, a white and chrome chair offers a spot for relaxation. Nearby, a black trash can with a white liner discreetly manages waste. Beyond, a hallway connects to the rest of the building, leading to various destinations." (App Version 1: Unreliable LLM integration, while partially descriptive, lacks crucial details and provides false information).
"The image shows an indoor room corner with a door at the center, fitted with a window that has a blue‒gray curtain tied in the middle. To the left, there is a small white table with various papers and a sanitizer bottle on top, above which is a suspended ceiling with a speaker. A step ladder, a trash can, and some informational pamphlets on the wall are also visible. To the right, there is a wall-mounted brochure holder with leaflets. The floor is a dark wood laminate. The setting appears to be a professional office or clinic waiting area" (App Version 2).
Enhanced descriptive accuracy with LLMs: Our studies show that incorporating LLMs allows us to leverage prompt engineering and RAG techniques to craft prompts that result in engaging and informative descriptions. The incorporation of GPT-4 and LLaMA LLMs improved the reliability and richness of descriptions, and the addition of multistep object detection and scene understanding tools allowed us to better align description outputs with priorities and needs specific to individuals with visual impairment. Through codesign workshops, the prompts and therefore the output descriptions were tailored to be more engaging, useful, and appealing to visually impaired users, ensuring the provision of valuable spatial information shaped by individual preferences.
Figure 6. Testing of AI navigation systems in codesign environments with research collaborators Ability2Access, Virginia Department for the Blind and Visually Impaired and individuals with vision impairment. Images by Andrew Gipe-Lazarou
5. CONTRIBUTIONS AND EMPIRICAL FINDINGS
This research substantiates the transformative integration of AI-driven navigation systems into accessible environments, anticipating significant shifts in architectural practice, policy formulation, and design pedagogy. Using AI-powered accessibility tools alongside participatory design methodologies, this study augmented the navigational capabilities and spatial autonomy of vision-impaired individuals; it deployed adaptive navigation systems that were refined iteratively by extensive user feedback and codesign principles, demonstrating marked enhancements in the users' capacity to comprehend their surroundings, navigate effectively, and interact with their environment.
We identify two broader impacts:
Impact on Design Practice and Education: The methodological framework of codesign workshops has deepened our understanding of the accessibility challenges inherent in current architectural pedagogy and practice. Direct interactions with vision-impaired participants have provided critical insights into the deficiencies of existing designs and identified innovative opportunities for improvement. This process has extended the educational scope of our work beyond this project, enriching the design community's comprehension of inclusive principles and advocating their integration into professional curricula and practices.
Design for AI Accessibility: Our research employed advanced machine learning models and computer vision techniques to significantly improve object recognition and environmental understanding. These advancements improve the accessibility of built environments, particularly for visually impaired individuals. We optimized the technologies through iterative testing and multimodel integration and provided real-time feedback through auditory and haptic outputs, significantly augmenting spatial awareness for vision-impaired users. Despite these advancements, the full integration of these AI systems into architectural designs remains a complex challenge, requiring further adaptation to ensure their seamless functionality and inclusivity.
5.1. FUTURE DIRECTIONS AND CONTINUED RESEARCH:
The ongoing refinement of the AI-driven navigation app through participatory design workshops will increase our comprehension of inclusive architectural strategies, fostering more robust engagement between stakeholders and design professionals. The research aims to broaden the scope of these technologies to encompass a wider range of disabilities, enhancing their utility and inclusiveness. Navigation in complex environments requires robust, expensive AI systems.
These systems must process large volumes of data to manage real-time changes and obstacles. A significant challenge is the seamless integration of AI technologies into diverse architectural environments via cost-efficient, generalizable techniques. Ensuring robustness and adaptability across various settings necessitates ongoing research and development. Despite advancements in generative AI, inherent biases remain a concern, as these models often reflect societal stereotypes from the vast corpora of real-world text on which they are trained. Their integration into intelligent process automation can benefit greatly from participatory design strategies. Additionally, expanding the scope of these systems to accommodate a broader range of disabilities will enhance their inclusivity and utility. The AI accessibility systems developed in this study significantly improve the accessibility of built environments for vision-impaired individuals. The high accuracy and reliability of contextual descriptions and simple navigation instructions, as indicated by user feedback, underscore their potential to transform how vision-impaired individuals interact with and experience their surroundings. Future efforts will focus on simplifying the integration of AI technologies into both new and existing architectural frameworks, enhancing cost-effectiveness and operational efficiency.
6. CONCLUSION
This study advances the design and integration of artificial intelligence (AI) systems into architectural practice to improve the accessibility of the built environment for individuals with vision impairment. Our collaborative efforts among architects, technologists, students, and community stakeholders have resulted in foundational principles for the codesign of AI-enabled systems that enhance the functional independence of users with disabilities.
This research has demonstrated emerging methods for accessing previously inaccessible spaces that are scalable and cost-effective. The results of our work encourage ongoing partnerships between the disability community and professionals in architecture and technology. Such collaboration is essential to ensure that AI technologies meet both the technical standards and practical needs of people with disabilities. Our findings highlight the importance of multidisciplinary collaboration in creating solutions that are innovative and inclusive. Looking ahead, it is crucial that these collaborations continue to grow, ensuring that accessibility is prioritized from a project's conception to its completion.
 7. ACKNOWLEDGEMENTS
Our research team would like to extend its gratitude to the Virginia Department for the Blind and Vision Impaired (DBVI), the Institute for Creativity, Arts, and Technology (ICAT) at Virginia Tech, the New River Valley Disability Resource Center (NRVDRC), and Ability2Access for their invaluable contributions to this research. This work was supported by the Virginia Tech Center for New Ventures and the Commonwealth Cyber Initiative (CCI Northern Virginia Node). Relevant data, models, or codes that support the findings of this study are available from the corresponding author upon reasonable request. The authors contributed equally to this study. 

ENDNOTES
1. Bauer, Zuria, et al. "Enhancing Perception for the Visually Impaired with Deep Learning Techniques and Low-Cost Wearable Sensors." Pattern Recognition Letters 137, no. 3 (September 2019): 27-36. DOI:10.1016/j.patrec.2019.03.008.
2. Bourne, Rupert R.A., et al. "Magnitude, Temporal Trends, and Projections of the Global Prevalence of Blindness and Distance and Near Vision Impairment: A Systematic Review and Meta-Analysis." The Lancet Global Health 5, no. 9 (September 2017): e888-e897. DOI:10.1016/S2214-109X(17)30293-0.
3. Budrionis, Andrius, et al. "Smartphone-Based Computer Vision Travelling Aids for Blind and Visually Impaired Individuals: A Systematic Review." Assistive Technology 34, no. 2 (2020): 178-194. DOI:10.1080/10400435.2020.1743381.
4. Chakraborti, Tathagata, et al. "From Robotic Process Automation to Intelligent 
Process Automation," in Business Process Management: Blockchain and Robotic Process Automation Forum, ed. Aleksandre Asatiani, Josep Carmona, and Remco Dijkman (Cham: Springer, 2020), 107-121.
5. Chui, Michael, et al. The Economic Potential of Generative AI: The Next Productivity Frontier (New York: McKinsey Global Institute, 2023).
6. Giudice, Nicholas A., and Gordon E. Legge, "Blind Navigation and the Role of Technology," in Engineering Handbook of Smart Technology for Aging, Disability, and Independence, ed. Abdelsalam Helal, Mounir Mokhtari, and Bessam Abdulrazak (Hoboken: John Wiley & Sons, 2008), 479-500.
7. Griffin-Shirley, Nora, et al. "A Survey on the Use of Mobile Applications for People Who are Visually Impaired." Journal of Visual Impairment & Blindness 114, no. 4 (July 2017): 307-323.
8. Kim, In-Ju. "Recent Advancements in Indoor Electronic Travel Aids for the Blind or Visually Impaired: A Comprehensive Review of Technologies and Implementations." Universal Access in the Information Society (2023). DOI:10.1007/s10209-023-01086-8.
9. Ko, Eunjeong, and Eun Yi Kim. "A Vision-Based Wayfinding System for Visually Impaired People Using Situation Awareness and Activity-Based Instructions." Sensors 17, no. 8 (August 2017). DOI:10.3390/s17081882.
10. Nadeem, Moin, Anna Bethke, and Siva Reddy. "StereoSet: Measuring 
Stereotypical Bias in Pretrained Language Models," in Proceedings of the 
59th Annual Meeting of the Association for Computational Linguistics and the 
11th International Joint Conference on Natural Language Processing (Online: Association for Computational Linguistics, 2021), 5356-5371.
11. Snyder, Joel. "Audio Description: Seeing with the Mind's Eye: A Comprehensive Training Manual and Guide to the History and Applications of Audio Description." PhD diss., Universitat Autònoma de Barcelona, 2014.
12. World Health Organization. World Report on Vision (Geneva: World Health Organization, 2019). https://www.who.int/publications/i/item/9789241516570.
13. Zafar, Sadia, et al. "Assistive Devices Analysis for Visually Impaired Persons: A Review on Taxonomy." IEEE Access 10 (2022): 13354-13366. DOI:10.1109/ access.2022.3146728.


2	"I WANT": AGENCY AND ACCESSIBILITY IN THE AGE OF AI

ACSA 2024 International Conference: Inflections | June 27-29, 2024 | Querétaro, Mexico	1

ACSA 2024 International Conference: Inflections | June 27-29, 2024 | Querétaro, Mexico	1

